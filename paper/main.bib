@article{zhang2018graph,
  title={Graph hypernetworks for neural architecture search},
  author={Zhang, Chris and Ren, Mengye and Urtasun, Raquel},
  journal={arXiv preprint arXiv:1810.05749},
  year={2018}
}

@article{wurtz2011neuronal,
  title={Neuronal mechanisms for visual stability: progress and problems},
  author={Wurtz, Robert H and Joiner, Wilsaan M and Berman, Rebecca A},
  journal={Philosophical Transactions of the Royal Society B: Biological Sciences},
  volume={366},
  number={1564},
  pages={492--503},
  year={2011},
  publisher={The Royal Society}
}

@article{digre2012shedding,
  title={Shedding light on photophobia},
  author={Digre, Kathleen B and Brennan, KC},
  journal={Journal of Neuro-ophthalmology},
  volume={32},
  number={1},
  pages={68--81},
  year={2012},
  publisher={LWW}
}

@inproceedings{he2022hyperprompt,
  title={Hyperprompt: Prompt-based task-conditioning of transformers},
  author={He, Yun and Zheng, Steven and Tay, Yi and Gupta, Jai and Du, Yu and Aribandi, Vamsi and Zhao, Zhe and Li, YaGuang and Chen, Zhao and Metzler, Donald and others},
  booktitle={International conference on machine learning},
  pages={8678--8690},
  year={2022},
  organization={PMLR}
}

@inproceedings{ivison2022hyperdecoders,
  title={Hyperdecoders: Instance-specific decoders for multi-task NLP},
  author={Ivison, Hamish and Peters, Matthew E},
  booktitle={Findings of the Association for Computational Linguistics: EMNLP 2022},
  pages={1715--1730},
  year={2022}
}


@article{mahabadi2021parameter,
  title={Parameter-efficient multi-task fine-tuning for transformers via shared hypernetworks},
  author={Mahabadi, Rabeeh Karimi and Ruder, Sebastian and Dehghani, Mostafa and Henderson, James},
  journal={arXiv preprint arXiv:2106.04489},
  year={2021}
}

@article{ortiz2024hyperloader,
  title={HyperLoader: Integrating Hypernetwork-Based LoRA and Adapter Layers into Multi-Task Transformers for Sequence Labelling},
  author={Ortiz-Barajas, Jesus-German and Gomez-Adorno, Helena and Solorio, Thamar},
  journal={arXiv preprint arXiv:2407.01411},
  year={2024}
}

@article{gururangan2020don,
  title={Don't stop pretraining: Adapt language models to domains and tasks},
  author={Gururangan, Suchin and Marasovi{\'c}, Ana and Swayamdipta, Swabha and Lo, Kyle and Beltagy, Iz and Downey, Doug and Smith, Noah A},
  journal={arXiv preprint arXiv:2004.10964},
  year={2020}
}

@article{wei2021finetuned,
  title={Finetuned language models are zero-shot learners},
  author={Wei, Jason and Bosma, Maarten and Zhao, Vincent Y and Guu, Kelvin and Yu, Adams Wei and Lester, Brian and Du, Nan and Dai, Andrew M and Le, Quoc V},
  journal={arXiv preprint arXiv:2109.01652},
  year={2021}
}

@article{dettmers2022gpt3,
  title={Gpt3. int8 (): 8-bit matrix multiplication for transformers at scale},
  author={Dettmers, Tim and Lewis, Mike and Belkada, Younes and Zettlemoyer, Luke},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={30318--30332},
  year={2022}
}

@article{tay2021scale,
  title={Scale efficiently: Insights from pre-training and fine-tuning transformers},
  author={Tay, Yi and Dehghani, Mostafa and Rao, Jinfeng and Fedus, William and Abnar, Samira and Chung, Hyung Won and Narang, Sharan and Yogatama, Dani and Vaswani, Ashish and Metzler, Donald},
  journal={arXiv preprint arXiv:2109.10686},
  year={2021}
}

@article{von2019continual,
  title={Continual learning with hypernetworks},
  author={Von Oswald, Johannes and Henning, Christian and Grewe, Benjamin F and Sacramento, Jo{\~a}o},
  journal={arXiv preprint arXiv:1906.00695},
  year={2019}
}

@article{schug2024attention,
  title={Attention as a Hypernetwork},
  author={Schug, Simon and Kobayashi, Seijin and Akram, Yassir and Sacramento, Jo{\~a}o and Pascanu, Razvan},
  journal={arXiv preprint arXiv:2406.05816},
  year={2024}
}

@inproceedings{
hu2022lora,
title={Lo{RA}: Low-Rank Adaptation of Large Language Models},
author={Edward J Hu and yelong shen and Phillip Wallis and Zeyuan Allen-Zhu and Yuanzhi Li and Shean Wang and Lu Wang and Weizhu Chen},
booktitle={International Conference on Learning Representations},
year={2022},
url={https://openreview.net/forum?id=nZeVKeeFYf9}
}

@article{panda2024lota,
  title={Lottery Ticket Adaptation: Mitigating Destructive Interference in LLMs},
  author={Panda, Ashwinee and Isik, Berivan and Qi, Xiangyu and Koyejo, Sanmi and Weissman, Tsachy and Mittal, Prateek},
  journal={arXiv preprint arXiv:2406.16797},
  year={2024}
}

@misc{huang2023lorahub,
    title={LoraHub: Efficient Cross-Task Generalization via Dynamic LoRA Composition}, 
    author={Chengsong Huang and Qian Liu and Bill Yuchen Lin and Tianyu Pang and Chao Du and Min Lin},
    year={2023},
    eprint={2307.13269},
    archivePrefix={arXiv},
    primaryClass={cs.CL}
}

@article{wu2024mole,
  title={Mixture of lora experts},
  author={Wu, Xun and Huang, Shaohan and Wei, Furu},
  journal={arXiv preprint arXiv:2404.13628},
  year={2024}
}

@inproceedings{wu2023pi_routing,
  title={$\pi$-Tuning: Transferring Multimodal Foundation Models with Optimal Multi-task Interpolation},
  author={Wu, Chengyue and Wang, Teng and Ge, Yixiao and Lu, Zeyu and Zhou, Ruisong and Shan, Ying and Luo, Ping},
  booktitle={International Conference on Machine Learning},
  pages={37713--37727},
  year={2023},
  organization={PMLR}
}

@article{ostapenko2024towards_modular_llms,
  title={Towards modular llms by building and reusing a library of loras},
  author={Ostapenko, Oleksiy and Su, Zhan and Ponti, Edoardo Maria and Charlin, Laurent and Roux, Nicolas Le and Pereira, Matheus and Caccia, Lucas and Sordoni, Alessandro},
  journal={arXiv preprint arXiv:2405.11157},
  year={2024}
}

@article{bruel2024compress_then_serve,
  title={Compress then Serve: Serving Thousands of LoRA Adapters with Little Overhead},
  author={Br{\"u}el-Gabrielsson, Rickard and Zhu, Jiacheng and Bhardwaj, Onkar and Choshen, Leshem and Greenewald, Kristjan and Yurochkin, Mikhail and Solomon, Justin},
  journal={arXiv preprint arXiv:2407.00066},
  year={2024}
}

@article{kim2024cond_lora,
  title={A Single Linear Layer Yields Task-Adapted Low-Rank Matrices},
  author={Kim, Hwichan and Sasaki, Shota and Hoshino, Sho and Honda, Ukyo},
  journal={arXiv preprint arXiv:2403.14946},
  year={2024}
}

@article{ha2016hypernetworks,
  title={Hypernetworks},
  author={Ha, David and Dai, Andrew and Le, Quoc V},
  journal={arXiv preprint arXiv:1609.09106},
  year={2016}
}

@article{stanley2002neat,
  title={Evolving neural networks through augmenting topologies},
  author={Stanley, Kenneth O and Miikkulainen, Risto},
  journal={Evolutionary computation},
  volume={10},
  number={2},
  pages={99--127},
  year={2002},
  publisher={MIT Press}
}

@article{schmidhuber1997discovering,
  title={Discovering neural nets with low Kolmogorov complexity and high generalization capability},
  author={Schmidhuber, J{\"u}rgen},
  journal={Neural Networks},
  volume={10},
  number={5},
  pages={857--873},
  year={1997},
  publisher={Elsevier}
}

@article{stanley2003taxonomy,
  title={A taxonomy for artificial embryogeny},
  author={Stanley, Kenneth O and Miikkulainen, Risto},
  journal={Artificial life},
  volume={9},
  number={2},
  pages={93--130},
  year={2003},
  publisher={MIT Press}
}

@inproceedings{wang2022sni,
  title={Super-NaturalInstructions: Generalization via Declarative Instructions on 1600+ NLP Tasks},
  author={Wang, Yizhong and Mishra, Swaroop and Alipoormolabashi, Pegah and Kordi, Yeganeh and Mirzaei, Amirreza and Naik, Atharva and Ashok, Arjun and Dhanasekaran, Arut Selvan and Arunkumar, Anjana and Stap, David and others},
  booktitle={Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing},
  pages={5085--5109},
  year={2022}
}

@article{jiang2023mistral,
  title={Mistral 7B},
  author={Jiang, Albert Q and Sablayrolles, Alexandre and Mensch, Arthur and Bamford, Chris and Chaplot, Devendra Singh and Casas, Diego de las and Bressand, Florian and Lengyel, Gianna and Lample, Guillaume and Saulnier, Lucile and others},
  journal={arXiv preprint arXiv:2310.06825},
  year={2023}
}

@article{zhang2024mgte,
  title={mGTE: Generalized Long-Context Text Representation and Reranking Models for Multilingual Text Retrieval},
  author={Zhang, Xin and Zhang, Yanzhao and Long, Dingkun and Xie, Wen and Dai, Ziqi and Tang, Jialong and Lin, Huan and Yang, Baosong and Xie, Pengjun and Huang, Fei and others},
  journal={arXiv preprint arXiv:2407.19669},
  year={2024}
}

@article{li2023towards,
  title={Towards general text embeddings with multi-stage contrastive learning},
  author={Li, Zehan and Zhang, Xin and Zhang, Yanzhao and Long, Dingkun and Xie, Pengjun and Zhang, Meishan},
  journal={arXiv preprint arXiv:2308.03281},
  year={2023}
}

@article{allenai:arc,
      author    = {Peter Clark  and Isaac Cowhey and Oren Etzioni and Tushar Khot and
                    Ashish Sabharwal and Carissa Schoenick and Oyvind Tafjord},
      title     = {Think you have Solved Question Answering? Try ARC, the AI2 Reasoning Challenge},
      journal   = {arXiv:1803.05457v1},
      year      = {2018},
}

@article{clark2019boolq,
  title={BoolQ: Exploring the surprising difficulty of natural yes/no questions},
  author={Clark, Christopher and Lee, Kenton and Chang, Ming-Wei and Kwiatkowski, Tom and Collins, Michael and Toutanova, Kristina},
  journal={arXiv preprint arXiv:1905.10044},
  year={2019}
}

@article{cobbe2021gsm8k,
  title={Training Verifiers to Solve Math Word Problems},
  author={Cobbe, Karl and Kosaraju, Vineet and Bavarian, Mohammad and Chen, Mark and Jun, Heewoo and Kaiser, Lukasz and Plappert, Matthias and Tworek, Jerry and Hilton, Jacob and Nakano, Reiichiro and Hesse, Christopher and Schulman, John},
  journal={arXiv preprint arXiv:2110.14168},
  year={2021}
}

@inproceedings{zellers2019hellaswag,
    title={HellaSwag: Can a Machine Really Finish Your Sentence?},
    author={Zellers, Rowan and Holtzman, Ari and Bisk, Yonatan and Farhadi, Ali and Choi, Yejin},
    booktitle ={Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics},
    year={2019}
}

@inproceedings{OpenBookQA2018,
 title={Can a Suit of Armor Conduct Electricity? A New Dataset for Open Book Question Answering},
 author={Todor Mihaylov and Peter Clark and Tushar Khot and Ashish Sabharwal},
 booktitle={EMNLP},
 year={2018}
}

@inproceedings{Bisk2020piqa,
  author = {Yonatan Bisk and Rowan Zellers and
            Ronan Le Bras and Jianfeng Gao
            and Yejin Choi},
  title = {PIQA: Reasoning about Physical Commonsense in
           Natural Language},
  booktitle = {Thirty-Fourth AAAI Conference on
               Artificial Intelligence},
  year = {2020},
}

@InProceedings{ai2:winogrande,
title = {WinoGrande: An Adversarial Winograd Schema Challenge at Scale},
author={Keisuke, Sakaguchi and Ronan, Le Bras and Chandra, Bhagavatula and Yejin, Choi
},
year={2019}
}


@misc{chen2021humaneval,
      title={Evaluating Large Language Models Trained on Code},
      author={Mark Chen and Jerry Tworek and Heewoo Jun and Qiming Yuan and Henrique Ponde de Oliveira Pinto and Jared Kaplan and Harri Edwards and Yuri Burda and Nicholas Joseph and Greg Brockman and Alex Ray and Raul Puri and Gretchen Krueger and Michael Petrov and Heidy Khlaaf and Girish Sastry and Pamela Mishkin and Brooke Chan and Scott Gray and Nick Ryder and Mikhail Pavlov and Alethea Power and Lukasz Kaiser and Mohammad Bavarian and Clemens Winter and Philippe Tillet and Felipe Petroski Such and Dave Cummings and Matthias Plappert and Fotios Chantzis and Elizabeth Barnes and Ariel Herbert-Voss and William Hebgen Guss and Alex Nichol and Alex Paino and Nikolas Tezak and Jie Tang and Igor Babuschkin and Suchir Balaji and Shantanu Jain and William Saunders and Christopher Hesse and Andrew N. Carr and Jan Leike and Josh Achiam and Vedant Misra and Evan Morikawa and Alec Radford and Matthew Knight and Miles Brundage and Mira Murati and Katie Mayer and Peter Welinder and Bob McGrew and Dario Amodei and Sam McCandlish and Ilya Sutskever and Wojciech Zaremba},
      year={2021},
      eprint={2107.03374},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@article{austin2021mbpp,
  title={Program Synthesis with Large Language Models},
  author={Austin, Jacob and Odena, Augustus and Nye, Maxwell and Bosma, Maarten and Michalewski, Henryk and Dohan, David and Jiang, Ellen and Cai, Carrie and Terry, Michael and Le, Quoc and others},
  journal={arXiv preprint arXiv:2108.07732},
  year={2021}
}

@article{achiam2023gpt,
  title={Gpt-4 technical report},
  author={Achiam, Josh and Adler, Steven and Agarwal, Sandhini and Ahmad, Lama and Akkaya, Ilge and Aleman, Florencia Leoni and Almeida, Diogo and Altenschmidt, Janko and Altman, Sam and Anadkat, Shyamal and others},
  journal={arXiv preprint arXiv:2303.08774},
  year={2023}
}

@article{zhao2024loraland,
  title={LoRA Land: 310 Fine-tuned LLMs that Rival GPT-4, A Technical Report},
  author={Zhao, Justin and Wang, Timothy and Abid, Wael and Angus, Geoffrey and Garg, Arnav and Kinnison, Jeffery and Sherstinsky, Alex and Molino, Piero and Addair, Travis and Rishi, Devvret},
  journal={arXiv preprint arXiv:2405.00732},
  year={2024}
}

@inproceedings{
kopiczko2024vera,
title={Ve{RA}: Vector-based Random Matrix Adaptation},
author={Dawid Jan Kopiczko and Tijmen Blankevoort and Yuki M Asano},
booktitle={The Twelfth International Conference on Learning Representations},
year={2024},
url={https://openreview.net/forum?id=NjNfLdxr3A}
}

@inproceedings{xiao-etal-2023-hyperlora-dialect,
    title = "Task-Agnostic Low-Rank Adapters for Unseen {E}nglish Dialects",
    author = "Xiao, Zedian  and
      Held, William  and
      Liu, Yanchen  and
      Yang, Diyi",
    editor = "Bouamor, Houda  and
      Pino, Juan  and
      Bali, Kalika",
    booktitle = "Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing",
    month = dec,
    year = "2023",
    address = "Singapore",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2023.emnlp-main.487",
    doi = "10.18653/v1/2023.emnlp-main.487",
    pages = "7857--7870",
}

@inproceedings{Lv2024hyperlora-few-shot,
  title={HyperloRA: Efficient cross-task generalization via constrained low-rank adapters generation},
  author={Lv, Chuancheng and Li, Lei and Zhang, Shitou and Chen, Gang and Qi, Fanchao and Zhang, Ningyu and Zheng, Hai-Tao},
  booktitle={Findings of the Association for Computational Linguistics: EMNLP 2024},
  pages={16376--16393},
  year={2024}
}

@inproceedings{
llm2vec,
title={{LLM2V}ec: Large Language Models Are Secretly Powerful Text Encoders},
author={Parishad BehnamGhader and Vaibhav Adlakha and Marius Mosbach and Dzmitry Bahdanau and Nicolas Chapados and Siva Reddy},
booktitle={First Conference on Language Modeling},
year={2024},
url={https://openreview.net/forum?id=IW1PR7vEBf}
}

@inproceedings{evalplus,
  title = {Is Your Code Generated by Chat{GPT} Really Correct? Rigorous Evaluation of Large Language Models for Code Generation},
  author = {Liu, Jiawei and Xia, Chunqiu Steven and Wang, Yuyao and Zhang, Lingming},
  booktitle = {Thirty-seventh Conference on Neural Information Processing Systems},
  year = {2023},
  url = {https://openreview.net/forum?id=1qvx610Cu7},
}

@inproceedings{phang2023hypertuning,
  title={Hypertuning: Toward adapting large language models without back-propagation},
  author={Phang, Jason and Mao, Yi and He, Pengcheng and Chen, Weizhu},
  booktitle={International Conference on Machine Learning},
  pages={27854--27875},
  year={2023},
  organization={PMLR}
}

@article{mu2024gisting,
  title={Learning to compress prompts with gist tokens},
  author={Mu, Jesse and Li, Xiang and Goodman, Noah},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  year={2024}
}

@inproceedings{deb-etal-2022-hnet-lm,
    title = "Boosting Natural Language Generation from Instructions with Meta-Learning",
    author = "Deb, Budhaditya  and
      Awadallah, Ahmed Hassan  and
      Zheng, Guoqing",
    editor = "Goldberg, Yoav  and
      Kozareva, Zornitsa  and
      Zhang, Yue",
    booktitle = "Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing",
    month = dec,
    year = "2022",
    address = "Abu Dhabi, United Arab Emirates",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.emnlp-main.456",
    doi = "10.18653/v1/2022.emnlp-main.456",
    pages = "6792--6808",
    abstract = "Recent work has shown that language models (LMs) trained with multi-task \textit{instructional learning} (MTIL) can solve diverse NLP tasks in zero- and few-shot settings with improved performance compared to prompt tuning. MTIL illustrates that LMs can extract and use information about the task from instructions beyond the surface patterns of the inputs and outputs. This suggests that meta-learning may further enhance the utilization of instructions for effective task transfer. In this paper we investigate whether meta-learning applied to MTIL can further improve generalization to unseen tasks in a zero-shot setting. Specifically, we propose to adapt meta-learning to MTIL in three directions: 1) Model Agnostic Meta Learning (MAML), 2) Hyper-Network (HNet) based adaptation to generate task specific parameters conditioned on instructions, and 3) an approach combining HNet and MAML. Through extensive experiments on the large scale Natural Instructions V2 dataset, we show that our proposed approaches significantly improve over strong baselines in zero-shot settings. In particular, meta-learning improves the effectiveness of instructions and is most impactful when the test tasks are strictly zero-shot (i.e. no similar tasks in the training set) and are {``}hard{''} for LMs, illustrating the potential of meta-learning for MTIL for out-of-distribution tasks.",
}

@inproceedings{ivison2023hint,
  title={HINT: Hypernetwork Instruction Tuning for Efficient Zero-and Few-Shot Generalisation},
  author={Ivison, Hamish and Bhagia, Akshita and Wang, Yizhong and Hajishirzi, Hannaneh and Peters, Matthew E},
  booktitle={Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
  pages={11272--11288},
  year={2023}
}

@article{raffel2020t5,
  title={Exploring the limits of transfer learning with a unified text-to-text transformer},
  author={Raffel, Colin and Shazeer, Noam and Roberts, Adam and Lee, Katherine and Narang, Sharan and Matena, Michael and Zhou, Yanqi and Li, Wei and Liu, Peter J},
  journal={Journal of machine learning research},
  volume={21},
  number={140},
  pages={1--67},
  year={2020}
}

@article{lewis2019bart,
  title={Bart: Denoising sequence-to-sequence pre-training for natural language generation, translation, and comprehension},
  author={Lewis, M},
  journal={arXiv preprint arXiv:1910.13461},
  year={2019}
}

@inproceedings{beck2023bias-hyperinit,
  title={Hypernetworks in meta-reinforcement learning},
  author={Beck, Jacob and Jackson, Matthew Thomas and Vuorio, Risto and Whiteson, Shimon},
  booktitle={Conference on Robot Learning},
  pages={1478--1487},
  year={2023},
  organization={PMLR}
}

@inproceedings{icl,
 author = {Brown, Tom and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared D and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and Agarwal, Sandhini and Herbert-Voss, Ariel and Krueger, Gretchen and Henighan, Tom and Child, Rewon and Ramesh, Aditya and Ziegler, Daniel and Wu, Jeffrey and Winter, Clemens and Hesse, Chris and Chen, Mark and Sigler, Eric and Litwin, Mateusz and Gray, Scott and Chess, Benjamin and Clark, Jack and Berner, Christopher and McCandlish, Sam and Radford, Alec and Sutskever, Ilya and Amodei, Dario},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {H. Larochelle and M. Ranzato and R. Hadsell and M.F. Balcan and H. Lin},
 pages = {1877--1901},
 publisher = {Curran Associates, Inc.},
 title = {Language Models are Few-Shot Learners},
 url = {https://proceedings.neurips.cc/paper_files/paper/2020/file/1457c0d6bfcb4967418bfb8ac142f64a-Paper.pdf},
 volume = {33},
 year = {2020}
}

@inproceedings{dong2024icl_survey,
  title={A survey on in-context learning},
  author={Dong, Qingxiu and Li, Lei and Dai, Damai and Zheng, Ce and Ma, Jingyuan and Li, Rui and Xia, Heming and Xu, Jingjing and Wu, Zhiyong and Chang, Baobao and others},
  booktitle={Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing},
  pages={1107--1128},
  year={2024}
}

@article{dubey2024llama,
  title={The llama 3 herd of models},
  author={Dubey, Abhimanyu and Jauhri, Abhinav and Pandey, Abhinav and Kadian, Abhishek and Al-Dahle, Ahmad and Letman, Aiesha and Mathur, Akhil and Schelten, Alan and Yang, Amy and Fan, Angela and others},
  journal={arXiv preprint arXiv:2407.21783},
  year={2024}
}

@article{team2024gemma,
  title={Gemma 2: Improving open language models at a practical size},
  author={Team, Gemma and Riviere, Morgane and Pathak, Shreya and Sessa, Pier Giuseppe and Hardin, Cassidy and Bhupatiraju, Surya and Hussenot, L{\'e}onard and Mesnard, Thomas and Shahriari, Bobak and Ram{\'e}, Alexandre and others},
  journal={arXiv preprint arXiv:2408.00118},
  year={2024}
}

@article{korthikanti2023reducing,
  title={Reducing activation recomputation in large transformer models},
  author={Korthikanti, Vijay Anand and Casper, Jared and Lym, Sangkug and McAfee, Lawrence and Andersch, Michael and Shoeybi, Mohammad and Catanzaro, Bryan},
  journal={Proceedings of Machine Learning and Systems},
  volume={5},
  pages={341--353},
  year={2023}
}
